{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba668fab",
   "metadata": {},
   "source": [
    "Maya Asher, 4/8/24\n",
    "# Reprocessing the Santa Barbara Corpus of Spoken American English\n",
    "- **NEW REPLACEMENT for progress report 2**\n",
    "- **EXISTING for progress report 3**\n",
    "\n",
    "The SBCSAE is a collection of time-aligned transcripts of audio files. Along with timestamps, the transcripts also include many non-alphabetic characters that denote different aspects of the speech. \n",
    "\n",
    "In this notebook, I process and clean up the raw text so that I can easily search for and locate my target words in my later analysis. Specifically, I read files into individual dfs and put them into a dictionary. I then separated them by column count and pickled them.\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae7fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38ad3c",
   "metadata": {},
   "source": [
    "## Read in files\n",
    "Originially, there were parsing errors with almost half the files, so I had to manually go through and fix some of the spacing in the files. I opened them in Atom, found the line with issues, and fixed the spacing, which usually just consisted of removing an extraneous tab.\n",
    "\n",
    "Now, all 43 TRN files are able to be read in and inserted into an individual Pandas df, which is stored in the dictionary `data_frames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63897cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with necessary files\n",
    "directory = \"/Users/mayaasher/data_science/Stance-Taking-in-Spontaneous-Speech/data/utf-16/\"\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# only files ending in .trn\n",
    "files = [file for file in files if file.endswith('.trn')]\n",
    "\n",
    "# sort files based on numerical order\n",
    "sorted_files = sorted(files, key=lambda x: int(x.split('.')[0][4:]))\n",
    "\n",
    "# dict to hold all dfs\n",
    "data_frames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdae1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all files IN ORDER!!!\n",
    "for file_name in sorted_files:\n",
    "    try:\n",
    "        filename = file_name\n",
    "        data = pd.read_csv(directory+filename, sep='\\t', header=None, encoding='utf-16-be')\n",
    "        df = pd.DataFrame(data)\n",
    "        data_frames[filename] = df\n",
    "        #data_frames.append(df)\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error parsing {filename}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be3b6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_frames.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e9a781",
   "metadata": {},
   "source": [
    "## Column issues\n",
    "Unfortunately, the dfs have varying numbers of columns (2, 3, and 4 columns). The 2 and 3 column dfs have timestamps that go to the hundredths place while the 4 column dfs go to the thousandths place. However, this will not be an issue as long as they are named uniformly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07cfc89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0                                         1\n",
      "0  0.00 2.53  FRED:                                    ... Okay.\n",
      "1   2.53 4.73                              One= large loan (Hx),\n",
      "2   4.73 6.23                                  ... renewed (Hx),\n",
      "3   6.23 8.08           ... a hundred ninety-seven= .. thousand,\n",
      "4   8.08 9.23                          a hundred eighty dollars.\n",
      "           0         1                                  2\n",
      "0  0.00 1.24  KEVIN:     Is that just [carbonated water]?\n",
      "1  0.45 1.24  WENDY:                      [No thank you].\n",
      "2  1.24 1.50                                        [2No,\n",
      "3  1.24 3.38  KEN:      [2(H) No this is2] crea=m [3soda.\n",
      "4  1.50 2.36  WENDY:                   It's cream soda2].\n",
      "       0      1        2                                                  3\n",
      "0  2.660  2.805  JOANNE:                                               But,\n",
      "1  2.805  4.685      NaN  so these slides <X should X> be real interesting.\n",
      "2  6.140  6.325     KEN:                                          ... Yeah.\n",
      "3  6.325  7.710      NaN                  I think it'll be real interesting\n",
      "4  7.710  8.535      NaN                           I think it'll be a real,\n"
     ]
    }
   ],
   "source": [
    "print(data_frames['SBC014.trn'].head())\n",
    "print(data_frames['SBC013.trn'].head())\n",
    "print(data_frames['SBC015.trn'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937ce4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBC001.trn (1312, 3)\n",
      "SBC002.trn (1419, 3)\n",
      "SBC003.trn (1546, 3)\n",
      "SBC004.trn (1298, 3)\n",
      "SBC005.trn (826, 3)\n",
      "SBC006.trn (1767, 3)\n",
      "SBC007.trn (731, 3)\n",
      "SBC008.trn (1496, 3)\n",
      "SBC009.trn (725, 3)\n",
      "SBC010.trn (1107, 3)\n",
      "SBC011.trn (996, 3)\n",
      "SBC013.trn (2259, 3)\n",
      "SBC014.trn (1189, 2)\n",
      "SBC015.trn (1984, 4)\n",
      "SBC016.trn (1518, 4)\n",
      "SBC017.trn (1169, 4)\n",
      "SBC018.trn (566, 4)\n",
      "SBC019.trn (1266, 4)\n",
      "SBC022.trn (705, 4)\n",
      "SBC023.trn (1518, 4)\n",
      "SBC024.trn (875, 4)\n",
      "SBC029.trn (1214, 4)\n",
      "SBC031.trn (1539, 4)\n",
      "SBC032.trn (1845, 4)\n",
      "SBC033.trn (818, 4)\n",
      "SBC034.trn (739, 4)\n",
      "SBC035.trn (1330, 4)\n",
      "SBC036.trn (1822, 4)\n",
      "SBC037.trn (978, 4)\n",
      "SBC042.trn (719, 4)\n",
      "SBC043.trn (1497, 4)\n",
      "SBC044.trn (1431, 4)\n",
      "SBC045.trn (1197, 4)\n",
      "SBC047.trn (1162, 4)\n",
      "SBC048.trn (1128, 4)\n",
      "SBC049.trn (1273, 4)\n",
      "SBC050.trn (959, 4)\n",
      "SBC051.trn (1681, 4)\n",
      "SBC056.trn (1600, 4)\n",
      "SBC057.trn (1012, 4)\n",
      "SBC058.trn (982, 4)\n",
      "SBC059.trn (1857, 4)\n",
      "SBC060.trn (1013, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in data_frames:\n",
    "    print(i, data_frames[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52063f32",
   "metadata": {},
   "source": [
    "## Column issue work-around\n",
    "For the sake of searching the dfs, I'm separating them out into 3 dicts depending on their column count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f69d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of keys for each dict\n",
    "all_keys_list = list(data_frames.keys())\n",
    "df2c_keys = all_keys_list[12]\n",
    "df3c_keys = all_keys_list[:12]\n",
    "df4c_keys = all_keys_list[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53058832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding keys and vals to each dict\n",
    "df3c = {key: data_frames[key] for key in df3c_keys if key in data_frames}\n",
    "df4c = {key: data_frames[key] for key in df4c_keys if key in data_frames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33eacf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to do this one manually\n",
    "df2c = {'SBC014.trn':data_frames['SBC014.trn']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71b5de6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SBC014.trn'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2c.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40c22b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SBC001.trn', 'SBC002.trn', 'SBC003.trn', 'SBC004.trn', 'SBC005.trn', 'SBC006.trn', 'SBC007.trn', 'SBC008.trn', 'SBC009.trn', 'SBC010.trn', 'SBC011.trn', 'SBC013.trn'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3c.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93ef432b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SBC015.trn', 'SBC016.trn', 'SBC017.trn', 'SBC018.trn', 'SBC019.trn', 'SBC022.trn', 'SBC023.trn', 'SBC024.trn', 'SBC029.trn', 'SBC031.trn', 'SBC032.trn', 'SBC033.trn', 'SBC034.trn', 'SBC035.trn', 'SBC036.trn', 'SBC037.trn', 'SBC042.trn', 'SBC043.trn', 'SBC044.trn', 'SBC045.trn', 'SBC047.trn', 'SBC048.trn', 'SBC049.trn', 'SBC050.trn', 'SBC051.trn', 'SBC056.trn', 'SBC057.trn', 'SBC058.trn', 'SBC059.trn', 'SBC060.trn'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4c.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "062d023c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 12 + 30 = 43\n"
     ]
    }
   ],
   "source": [
    "# looks good!\n",
    "print(len(df2c),'+', len(df3c), '+', len(df4c), '=', (len(df2c)+len(df3c)+len(df4c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d5495",
   "metadata": {},
   "source": [
    "## Rename columns and replace NaNs\n",
    "They have slightly different column names, but they all have the text column which is the most important for our purposes. Also, there are NaNs that are causing issues, so they are getting replaced with nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d729f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_2 = {0:'time_speaker', 1:'text'}\n",
    "column_names_3 = {0:'time', 1:'speaker', 2:'text'}\n",
    "column_names_4 = {0:'time_start', 1:'time_end', 2:'speaker', 3:'text'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f3d4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each dict to rename columns of each df\n",
    "for key, value in df2c.items():\n",
    "    df2c[key] = df2c[key].rename(columns=column_names_2)\n",
    "    df2c[key] = df2c[key].replace([np.nan], '')\n",
    "for key, value in df3c.items():\n",
    "    df3c[key] = df3c[key].rename(columns=column_names_3)\n",
    "    df3c[key] = df3c[key].replace([np.nan], '')\n",
    "for key, value in df4c.items():\n",
    "    df4c[key] = df4c[key].rename(columns=column_names_4)\n",
    "    df4c[key] = df4c[key].replace([np.nan], '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066651e",
   "metadata": {},
   "source": [
    "## Concatenate dictionaries\n",
    "Depending on the method I take for the analysis, having a dictionary of all the data frames may be helpful (especially now that the columns are labeled semi-uniformly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3911be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dict = {**df3c, **df2c, **df4c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eec191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking good...\n",
    "len(concatenated_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028b6ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SBC001.trn', 'SBC002.trn', 'SBC003.trn', 'SBC004.trn', 'SBC005.trn', 'SBC006.trn', 'SBC007.trn', 'SBC008.trn', 'SBC009.trn', 'SBC010.trn', 'SBC011.trn', 'SBC013.trn', 'SBC014.trn', 'SBC015.trn', 'SBC016.trn', 'SBC017.trn', 'SBC018.trn', 'SBC019.trn', 'SBC022.trn', 'SBC023.trn', 'SBC024.trn', 'SBC029.trn', 'SBC031.trn', 'SBC032.trn', 'SBC033.trn', 'SBC034.trn', 'SBC035.trn', 'SBC036.trn', 'SBC037.trn', 'SBC042.trn', 'SBC043.trn', 'SBC044.trn', 'SBC045.trn', 'SBC047.trn', 'SBC048.trn', 'SBC049.trn', 'SBC050.trn', 'SBC051.trn', 'SBC056.trn', 'SBC057.trn', 'SBC058.trn', 'SBC059.trn', 'SBC060.trn'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perfect!\n",
    "concatenated_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db8923",
   "metadata": {},
   "source": [
    "## Pickle\n",
    "I'm only pickling the dicts of ALL dfs so it can be used in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "287550d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('all_dfs.pkl', 'wb')\n",
    "pickle.dump(concatenated_dict, f, -1)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
