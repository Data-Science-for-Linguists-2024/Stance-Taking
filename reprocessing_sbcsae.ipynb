{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba668fab",
   "metadata": {},
   "source": [
    "Maya Asher, 4/8/24\n",
    "# Reprocessing the Santa Barbara Corpus of Spoken American English\n",
    "- **NEW REPLACEMENT for progress report 2**\n",
    "- **EXISTING for progress report 3**\n",
    "\n",
    "The SBCSAE is a collection of time-aligned transcripts of audio files. Along with timestamps, the transcripts also include many non-alphabetic characters that denote different aspects of the speech. \n",
    "\n",
    "In this notebook, I process and clean up the raw text so that I can easily search for and locate my target words in my later analysis. Specifically, I read files into individual dfs and put them into a dictionary. I then separated them by column count and pickled them.\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae7fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccbe28e",
   "metadata": {},
   "source": [
    "## Read in files\n",
    "Originially, there were parsing errors with almost half the files, so I had to manually go through and fix some of the spacing in the files. I opened them in Atom, found the line with issues, and fixed the spacing, which usually just consisted of removing an extraneous tab.\n",
    "\n",
    "Now, all 43 TRN files are able to be read in and inserted into an individual Pandas df, which is stored in the dictionary `data_frames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1ca24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder with necessary files\n",
    "directory = \"/Users/mayaasher/data_science/Stance-Taking-in-Spontaneous-Speech/data/utf-16/\"\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# only files ending in .trn\n",
    "files = [file for file in files if file.endswith('.trn')]\n",
    "\n",
    "# sort files based on numerical order\n",
    "sorted_files = sorted(files, key=lambda x: int(x.split('.')[0][4:]))\n",
    "\n",
    "# dict to hold all dfs\n",
    "data_frames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cbcc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all files IN ORDER!!!\n",
    "for file_name in sorted_files:\n",
    "    try:\n",
    "        filename = file_name\n",
    "        data = pd.read_csv(directory+filename, sep='\\t', header=None, encoding='utf-16-be')\n",
    "        df = pd.DataFrame(data)\n",
    "        data_frames[filename] = df\n",
    "        #data_frames.append(df)\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error parsing {filename}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f353d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_frames.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effe2b1",
   "metadata": {},
   "source": [
    "## Column issues\n",
    "Unfortunately, the dfs have varying numbers of columns (2, 3, and 4 columns). The 2 and 3 column dfs have timestamps that go to the hundredths place while the 4 column dfs go to the thousandths place, so perhaps that caused processing issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1206a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0                                         1\n",
      "0  0.00 2.53  FRED:                                    ... Okay.\n",
      "1   2.53 4.73                              One= large loan (Hx),\n",
      "2   4.73 6.23                                  ... renewed (Hx),\n",
      "3   6.23 8.08           ... a hundred ninety-seven= .. thousand,\n",
      "4   8.08 9.23                          a hundred eighty dollars.\n",
      "           0         1                                  2\n",
      "0  0.00 1.24  KEVIN:     Is that just [carbonated water]?\n",
      "1  0.45 1.24  WENDY:                      [No thank you].\n",
      "2  1.24 1.50                                        [2No,\n",
      "3  1.24 3.38  KEN:      [2(H) No this is2] crea=m [3soda.\n",
      "4  1.50 2.36  WENDY:                   It's cream soda2].\n",
      "       0      1        2                                                  3\n",
      "0  2.660  2.805  JOANNE:                                               But,\n",
      "1  2.805  4.685      NaN  so these slides <X should X> be real interesting.\n",
      "2  6.140  6.325     KEN:                                          ... Yeah.\n",
      "3  6.325  7.710      NaN                  I think it'll be real interesting\n",
      "4  7.710  8.535      NaN                           I think it'll be a real,\n"
     ]
    }
   ],
   "source": [
    "print(data_frames['SBC014.trn'].head())\n",
    "print(data_frames['SBC013.trn'].head())\n",
    "print(data_frames['SBC015.trn'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937ce4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBC001.trn (1312, 3)\n",
      "SBC002.trn (1419, 3)\n",
      "SBC003.trn (1546, 3)\n",
      "SBC004.trn (1298, 3)\n",
      "SBC005.trn (826, 3)\n",
      "SBC006.trn (1767, 3)\n",
      "SBC007.trn (731, 3)\n",
      "SBC008.trn (1496, 3)\n",
      "SBC009.trn (725, 3)\n",
      "SBC010.trn (1107, 3)\n",
      "SBC011.trn (996, 3)\n",
      "SBC013.trn (2259, 3)\n",
      "SBC014.trn (1189, 2)\n",
      "SBC015.trn (1984, 4)\n",
      "SBC016.trn (1518, 4)\n",
      "SBC017.trn (1169, 4)\n",
      "SBC018.trn (566, 4)\n",
      "SBC019.trn (1266, 4)\n",
      "SBC022.trn (705, 4)\n",
      "SBC023.trn (1518, 4)\n",
      "SBC024.trn (875, 4)\n",
      "SBC029.trn (1214, 4)\n",
      "SBC031.trn (1539, 4)\n",
      "SBC032.trn (1845, 4)\n",
      "SBC033.trn (818, 4)\n",
      "SBC034.trn (739, 4)\n",
      "SBC035.trn (1330, 4)\n",
      "SBC036.trn (1822, 4)\n",
      "SBC037.trn (978, 4)\n",
      "SBC042.trn (719, 4)\n",
      "SBC043.trn (1497, 4)\n",
      "SBC044.trn (1431, 4)\n",
      "SBC045.trn (1197, 4)\n",
      "SBC047.trn (1162, 4)\n",
      "SBC048.trn (1128, 4)\n",
      "SBC049.trn (1273, 4)\n",
      "SBC050.trn (959, 4)\n",
      "SBC051.trn (1681, 4)\n",
      "SBC056.trn (1600, 4)\n",
      "SBC057.trn (1012, 4)\n",
      "SBC058.trn (982, 4)\n",
      "SBC059.trn (1857, 4)\n",
      "SBC060.trn (1013, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in data_frames:\n",
    "    print(i, data_frames[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32486405",
   "metadata": {},
   "source": [
    "## Column issue work-around\n",
    "For the sake of searching the dfs, I'm separating them out into 3 dicts depending on their column count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de22cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys_list = list(data_frames.keys())\n",
    "df2c = all_keys_list[12]\n",
    "df3c = all_keys_list[:12]\n",
    "df4c = all_keys_list[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ed1113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# only 1 in df2c\n",
    "print(len(df3c))\n",
    "print(len(df4c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41638dc",
   "metadata": {},
   "source": [
    "## Pickle\n",
    "I'm pickling the dicts of dfs so they can be used in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5341be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('df2col.pkl', 'wb')\n",
    "pickle.dump(df2c, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab4734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('df3col.pkl', 'wb')\n",
    "pickle.dump(df3c, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab114b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('df4col.pkl', 'wb')\n",
    "pickle.dump(df4c, f, -1)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
